{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb40f36c-072c-44b0-a7e1-bee6ff45182a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"material_master\"\n",
    "VOLUME  = \"datastore\"   # <-- includes the space exactly\n",
    "\n",
    "base = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/\"\n",
    "display(dbutils.fs.ls(base))  # you should see material_master_1k.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6011470-adb2-47fa-9ff2-e2e81618a230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT notebook: dlt_material_master\n",
    "# Catalog/Schema/Volume based on your setup:\n",
    "#   catalog  = workspace\n",
    "#   schema   = material_master\n",
    "#   volume   = \"datastore\"   \n",
    "#   file     = material_master_1k.csv\n",
    "\n",
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"material_master\"\n",
    "VOLUME  = \"datastore\"\n",
    "FILE    = \"material_master_1k.csv\"\n",
    "\n",
    "SOURCE_CSV = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{FILE}\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BRONZE — RAW AS-IS (CSV|)\n",
    "# =========================\n",
    "@dlt.table(\n",
    "    name=\"material_master_bronze\",\n",
    "    comment=\"Bronze: raw pipe-delimited Material Master from Volume (as-is).\"\n",
    ")\n",
    "def material_master_bronze():\n",
    "    return (\n",
    "        spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"delimiter\", \"|\")\n",
    "            .csv(SOURCE_CSV)\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SILVER — CLEAN, STANDARDIZE, FIX DATATYPES\n",
    "# - trims strings\n",
    "# - normalizes column names to lower_snake\n",
    "# - casts numeric/date fields\n",
    "# - drops bad rows (incl. expectation)\n",
    "# ============================================\n",
    "@dlt.table(\n",
    "    name=\"material_master_silver\",\n",
    "    comment=\"Silver: cleaned & standardized Material Master with datatype fixes and row-level validation.\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"not_null_material_id\", \"material_id IS NOT NULL\")   # required expectation\n",
    "def material_master_silver():\n",
    "    df = dlt.read(\"material_master_bronze\")\n",
    "\n",
    "    # 1) Normalize column names to lower-case once\n",
    "    df = df.select([F.col(c).alias(c.lower()) for c in df.columns])\n",
    "\n",
    "    # 2) Trim all string columns\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "    # 3) Datatype fixes (apply only if the columns exist in your file)\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    # ---- Numeric examples ----\n",
    "    # price -> DECIMAL(18,2)\n",
    "    if \"price\" in cols:\n",
    "        df = (df\n",
    "              .withColumn(\"price_raw\", F.col(\"price\"))\n",
    "              .withColumn(\"price\", F.col(\"price\").cast(\"decimal(18,2)\"))\n",
    "             )\n",
    "        # Drop rows where a non-null raw price failed to cast (invalid type)\n",
    "        df = df.filter(~(F.col(\"price_raw\").isNotNull() & F.col(\"price\").isNull()))\n",
    "        # Optional: keep only non-negative prices\n",
    "        df = df.filter((F.col(\"price\").isNull()) | (F.col(\"price\") >= F.lit(0)))\n",
    "\n",
    "    # weight -> DOUBLE\n",
    "    if \"weight\" in cols:\n",
    "        df = (df\n",
    "              .withColumn(\"weight_raw\", F.col(\"weight\"))\n",
    "              .withColumn(\"weight\", F.col(\"weight\").cast(\"double\"))\n",
    "             )\n",
    "        df = df.filter(~(F.col(\"weight_raw\").isNotNull() & F.col(\"weight\").isNull()))\n",
    "\n",
    "    # ---- Date examples ----\n",
    "    # Try common date columns in order; cast the first one that exists\n",
    "    for dcol in [\"created_date\", \"create_date\", \"effective_date\", \"date\"]:\n",
    "        if dcol in cols:\n",
    "            # Adjust the format if yours differs (e.g., \"MM/dd/yyyy\")\n",
    "            df = df.withColumn(dcol, F.to_date(F.col(dcol), \"yyyy-MM-dd\"))\n",
    "            # If a non-null raw value failed to parse, drop that row\n",
    "            df = df.filter(~(F.col(dcol).isNotNull() & F.to_date(F.col(dcol), \"yyyy-MM-dd\").isNull()))\n",
    "            break\n",
    "\n",
    "    # 4) (Already handled by expectation) material_id must be NOT NULL.\n",
    "    #    You can add more rules as expectations if you want, e.g.:\n",
    "    # @dlt.expect(\"valid_price_nonnegative\", \"price >= 0\")  # optional\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973bb391-0630-4fb2-a5f0-e184dda6fb34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt_material_master",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
